{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaEGru_My26H"
      },
      "source": [
        "# Replace openaiAPI.txt with your Open AI api key and githubapi.txt with your GitHub api key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGu74PmwyoCT",
        "outputId": "7cbb5d30-8fa2-492d-d6bf-28277f1d75e2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def read_file(filename):\n",
        "    \"\"\"Read contents of a file.\"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        return f.read().strip()\n",
        "\n",
        "\n",
        "def save_file(filename, content):\n",
        "    \"\"\"Save content to a file.\"\"\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(content.strip())\n",
        "\n",
        "\n",
        "def extract_resume_content(pdf_path):\n",
        "    \"\"\"Extract the content from the resume PDF and save it to 'resume.txt'.\"\"\"\n",
        "    pdf_loader = PyPDFLoader(pdf_path)\n",
        "    resume_text = \"\\n\".join(page.page_content for page in pdf_loader.load())\n",
        "    save_file('resume.txt', resume_text)\n",
        "    print(\"Resume content has been saved to 'resume.txt'.\")\n",
        "    return resume_text\n",
        "\n",
        "\n",
        "def extract_project_names_with_langchain(text):\n",
        "    \"\"\"Extract project names and save them to 'proj_names.txt'.\"\"\"\n",
        "    os.environ['OPENAI_API_KEY'] = read_file('openaiAPI.txt')\n",
        "    llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo')\n",
        "\n",
        "    names_prompt = PromptTemplate(\n",
        "        input_variables=['text'],\n",
        "        template=\"\"\"Extract only the project names from the following text.\n",
        "        Return only the names, one per line, without any descriptions or bullet points.\n",
        "        Remove any numbering or special characters from the start of the names.\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Project Names:\"\"\"\n",
        "    )\n",
        "\n",
        "    names_chain = LLMChain(llm=llm, prompt=names_prompt)\n",
        "    project_names = names_chain.run(text)\n",
        "    save_file('proj_names.txt', project_names)\n",
        "    print(\"Project names have been extracted and saved to 'proj_names.txt'.\")\n",
        "    return project_names.strip().split('\\n')\n",
        "\n",
        "\n",
        "def extract_project_details_with_langchain(text):\n",
        "    \"\"\"Extract project names and descriptions and save them to 'projects.txt'.\"\"\"\n",
        "    os.environ['OPENAI_API_KEY'] = read_file('openaiAPI.txt')\n",
        "    llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo')\n",
        "\n",
        "    projects_prompt = PromptTemplate(\n",
        "        input_variables=['text'],\n",
        "        template=\"\"\"Extract the full project details including the names and their descriptions from the following text.\n",
        "        Keep the structure intact with project names followed by their descriptions.\n",
        "        Remove any numbering or bullet points from the project names.\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Projects:\"\"\"\n",
        "    )\n",
        "\n",
        "    projects_chain = LLMChain(llm=llm, prompt=projects_prompt)\n",
        "    project_details = projects_chain.run(text)\n",
        "    save_file('projects.txt', project_details)\n",
        "    print(\"Project details have been extracted and saved to 'projects.txt'.\")\n",
        "    return project_details\n",
        "\n",
        "\n",
        "def extract_skills_with_langchain(text):\n",
        "    \"\"\"Extract skills and save them to 'skills.txt'.\"\"\"\n",
        "    os.environ['OPENAI_API_KEY'] = read_file('openaiAPI.txt')\n",
        "    llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo')\n",
        "\n",
        "    skills_prompt = PromptTemplate(\n",
        "        input_variables=['text'],\n",
        "        template=\"\"\"Extract all skills mentioned in the following text.\n",
        "        Return only the skills as a comma-separated list.\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Skills:\"\"\"\n",
        "    )\n",
        "\n",
        "    skills_chain = LLMChain(llm=llm, prompt=skills_prompt)\n",
        "    skills = skills_chain.run(text)\n",
        "    save_file('skills.txt', skills)\n",
        "    print(\"Skills have been extracted and saved to 'skills.txt'.\")\n",
        "    return skills.strip()\n",
        "\n",
        "def get_languages(owner, repo, headers):\n",
        "    \"\"\"Get the languages used in a specific GitHub repository.\"\"\"\n",
        "    api_url = f'https://api.github.com/repos/{owner}/{repo}/languages'\n",
        "    response = requests.get(api_url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    return response.json()\n",
        "\n",
        "def get_all_repos(user, headers):\n",
        "    \"\"\"Fetch all repositories of a GitHub user.\"\"\"\n",
        "    api_url = f'https://api.github.com/users/{user}/repos?per_page=100'\n",
        "    repos = []\n",
        "\n",
        "    while api_url:\n",
        "        response = requests.get(api_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        repos.extend(response.json())\n",
        "\n",
        "        # Check for pagination and fetch next page if available\n",
        "        api_url = response.links.get('next', {}).get('url')\n",
        "\n",
        "    return repos\n",
        "\n",
        "def fetch_and_save_github_info(user, headers, output_filename):\n",
        "    \"\"\"Fetch repository details and save to a text file.\"\"\"\n",
        "    repos = get_all_repos(user, headers)\n",
        "    print(repos)\n",
        "\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "        for repo in repos:\n",
        "            repo_name = repo['name']\n",
        "            repo_url = repo['html_url']\n",
        "            print(f\"Processing repository: {repo_name}\")\n",
        "\n",
        "            # Fetch languages used in the repository\n",
        "            languages = get_languages(repo['owner']['login'], repo_name, headers)\n",
        "\n",
        "            # Write repository details to file\n",
        "            f.write(f\"Repository: {repo_name}\\n\")\n",
        "            for language, size in languages.items():\n",
        "                f.write(f\"{language}: {size} bytes\\n\")\n",
        "            f.write(f\"Repository URL: {repo_url}\\n\")\n",
        "            f.write(f\"{'=' * 80}\\n\")\n",
        "\n",
        "    print(f\"Repository details saved to {output_filename}\")\n",
        "\n",
        "\n",
        "# Step 1: Extract resume content from PDF and save to 'resume.txt'\n",
        "pdf_path = 'Saurabh_resume.pdf'  # Replace with your PDF file name\n",
        "resume_text = extract_resume_content(pdf_path)\n",
        "\n",
        "# Step 2: Extract project names from resume and save to 'proj_names.txt'\n",
        "project_names = extract_project_names_with_langchain(resume_text)\n",
        "\n",
        "# Step 3: Extract project details (names + descriptions) and save to 'projects.txt'\n",
        "project_details = extract_project_details_with_langchain(resume_text)\n",
        "\n",
        "# Step 4: Extract skills from resume and save to 'skills.txt'\n",
        "skills = extract_skills_with_langchain(resume_text)\n",
        "\n",
        "# Step 5: Fetch GitHub repository information and save to 'github_lang.txt'\n",
        "github_token = read_file('githubapi.txt')\n",
        "headers = {\n",
        "    'Authorization': f'token {github_token}',\n",
        "    'Accept': 'application/vnd.github.v3+json'\n",
        "}\n",
        "github_user = input(\"Enter GitHub Username: \")\n",
        "fetch_and_save_github_info(github_user, headers, 'github_lang.txt')\n",
        "\n",
        "print(\"All tasks completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOtMROQFyoCY",
        "outputId": "5cd2bdc6-1596-4b53-d34a-f162dbaefd59"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import base64\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "import requests\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def read_file(filename):\n",
        "    \"\"\"Read contents of a file.\"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        return f.read().strip()\n",
        "\n",
        "def extract_sections_with_langchain(text):\n",
        "    \"\"\"\n",
        "    Extract project names using LangChain.\n",
        "    Saves them in separate files.\n",
        "    \"\"\"\n",
        "    # Set OpenAI API key\n",
        "    os.environ['OPENAI_API_KEY'] = read_file('openaiAPI.txt')\n",
        "\n",
        "    # Initialize language model\n",
        "    llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo')\n",
        "\n",
        "    # Create prompts for extraction\n",
        "    names_prompt = PromptTemplate(\n",
        "        input_variables=['text'],\n",
        "        template=\"\"\"\n",
        "        Extract only the project names from the following text.\n",
        "        Return only the names, one per line, without any descriptions or bullet points.\n",
        "        Remove any numbering or special characters from the start of the names.\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Project Names:\"\"\"\n",
        "    )\n",
        "\n",
        "    descriptions_prompt = PromptTemplate(\n",
        "        input_variables=['text'],\n",
        "        template=\"\"\"\n",
        "        Extract the projects section with full descriptions from the following text.\n",
        "        Include the project names and their complete descriptions.\n",
        "\n",
        "        Text:\n",
        "        {text}\n",
        "\n",
        "        Projects:\"\"\"\n",
        "    )\n",
        "\n",
        "    # Create chains\n",
        "    names_chain = LLMChain(llm=llm, prompt=names_prompt)\n",
        "    descriptions_chain = LLMChain(llm=llm, prompt=descriptions_prompt)\n",
        "\n",
        "    # Extract content\n",
        "    project_names = names_chain.run(text)\n",
        "    project_descriptions = descriptions_chain.run(text)\n",
        "\n",
        "    # Save to files\n",
        "    with open('proj_names.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(project_names.strip())\n",
        "\n",
        "    with open('projects.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(project_descriptions.strip())\n",
        "\n",
        "    return project_names.strip().split('\\n')\n",
        "\n",
        "def get_proj_names(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    names = [line.strip() for line in lines]\n",
        "    print(names)\n",
        "    return names\n",
        "\n",
        "def get_repositories(user, headers, project_names, similarity_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Fetch all repositories of a GitHub user and return those with names\n",
        "    similar to the provided project names based on cosine similarity.\n",
        "\n",
        "    Args:\n",
        "        user (str): GitHub username.\n",
        "        headers (dict): Headers for the API request (e.g., authentication token).\n",
        "        project_names (list of str): List of project names to match against.\n",
        "        similarity_threshold (float): Minimum similarity score for a repository to be included.\n",
        "\n",
        "    Returns:\n",
        "        list of dict: Repositories with similar names and their owner details.\n",
        "    \"\"\"\n",
        "    api_url = f'https://api.github.com/users/{user}/repos?per_page=100'\n",
        "    repos = []\n",
        "    repo_names = []\n",
        "\n",
        "    # Fetch all repositories\n",
        "    while api_url:\n",
        "        response = requests.get(api_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        repos.extend(response.json())\n",
        "        api_url = response.links.get('next', {}).get('url')  # Pagination\n",
        "\n",
        "    # Extract repository names\n",
        "    repo_names = [repo['name'] for repo in repos]\n",
        "\n",
        "    # Combine provided project names and repository names for vectorization\n",
        "    all_names = project_names + repo_names\n",
        "\n",
        "    # Compute TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer().fit_transform(all_names)\n",
        "\n",
        "    # Compute cosine similarity between project names and repository names\n",
        "    project_vectors = vectorizer[:len(project_names)]\n",
        "    repo_vectors = vectorizer[len(project_names):]\n",
        "    similarity_matrix = cosine_similarity(project_vectors, repo_vectors)\n",
        "\n",
        "    # Find repositories with similarity above the threshold\n",
        "    similar_repos = []\n",
        "    for project_idx, project_name in enumerate(project_names):\n",
        "        for repo_idx, similarity in enumerate(similarity_matrix[project_idx]):\n",
        "            if similarity >= similarity_threshold:\n",
        "                repo = repos[repo_idx]\n",
        "                similar_repos.append({\n",
        "                    'project_name': project_name,\n",
        "                    'repo_name': repo['name'],\n",
        "                    'owner_login': repo['owner']['login'],\n",
        "                    'similarity': similarity,\n",
        "                    'repo_data': repo\n",
        "                })\n",
        "\n",
        "    return similar_repos\n",
        "\n",
        "def get_repo_contents(owner, repo, path='', headers=None):\n",
        "    \"\"\"\n",
        "    Recursively fetch code files from a repository using GitHub API.\n",
        "    \"\"\"\n",
        "    contents = []\n",
        "    allowed_extensions = {'.py', '.js', '.java', '.cpp', '.c', '.html', '.css', '.ts', '.go', '.rb', '.php'}  # Add other extensions as needed\n",
        "    api_url = f'https://api.github.com/repos/{owner}/{repo}/contents/{path}'\n",
        "\n",
        "    try:\n",
        "        response = requests.get(api_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        items = response.json()\n",
        "\n",
        "        # Handle single file response\n",
        "        if not isinstance(items, list):\n",
        "            items = [items]\n",
        "\n",
        "        for item in items:\n",
        "            if item['type'] == 'file':\n",
        "                file_extension = item['name'].split('.')[-1] if '.' in item['name'] else ''\n",
        "                if f\".{file_extension}\" not in allowed_extensions:\n",
        "                    print(f\"Skipping non-code file: {item['path']}\")\n",
        "                    continue\n",
        "\n",
        "                if item['size'] == 0:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    if item.get('size', 0) > 1000000:  # Skip files larger than 1MB\n",
        "                        print(f\"Skipping large file: {item['path']}\")\n",
        "                        continue\n",
        "\n",
        "                    raw_response = requests.get(item['download_url'], headers=headers)\n",
        "                    raw_response.raise_for_status()\n",
        "                    content = raw_response.text\n",
        "\n",
        "                    contents.append({\n",
        "                        'path': item['path'],\n",
        "                        'content': content\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error fetching content for {item['path']}: {e}\")\n",
        "\n",
        "            elif item['type'] == 'dir':\n",
        "                contents.extend(get_repo_contents(owner, repo, item['path'], headers))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing {api_url}: {e}\")\n",
        "        return contents\n",
        "\n",
        "    return contents\n",
        "\n",
        "def parse_github_lang_file(filename):\n",
        "    \"\"\"Parse github_lang.txt to extract repository URLs.\"\"\"\n",
        "    repo_info = {}\n",
        "    current_repo = None\n",
        "\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('Repository: '):\n",
        "            current_repo = line.replace('Repository: ', '')\n",
        "        elif line.startswith('Repository URL: ') and current_repo:\n",
        "            repo_info[current_repo] = line.replace('Repository URL: ', '')\n",
        "\n",
        "    return repo_info\n",
        "\n",
        "def extract_owner_repo(repo_url):\n",
        "    \"\"\"Extract owner and repo name from GitHub URL.\"\"\"\n",
        "    parts = repo_url.rstrip('/').split('/')\n",
        "    return parts[-2], parts[-1]\n",
        "\n",
        "# Load and read resume from PDF\n",
        "pdf_loader = PyPDFLoader('Saurabh_resume.pdf')\n",
        "resume_text = \"\\n\".join(page.page_content for page in pdf_loader.load())\n",
        "\n",
        "# Extract and save project names and descriptions using LangChain\n",
        "project_names = extract_sections_with_langchain(resume_text)\n",
        "print(\"Extracted project names:\", project_names)\n",
        "\n",
        "# Load GitHub API token\n",
        "github_token = read_file('githubapi.txt')\n",
        "headers = {\n",
        "    'Authorization': f'token {github_token}',\n",
        "    'Accept': 'application/vnd.github.v3+json'\n",
        "}\n",
        "\n",
        "proj_names = get_proj_names('proj_names.txt')\n",
        "headers = {\n",
        "    'Authorization': f'token {github_token}',\n",
        "    'Accept': 'application/vnd.github.v3+json'\n",
        "}\n",
        "\n",
        "# Parse repository information from github_lang.txt\n",
        "repo_info = get_repositories(github_user, headers, proj_names)\n",
        "print(\"Available repositories:\", list(repo_info))\n",
        "\n",
        "# Create/open resume_proj_code.txt for writing\n",
        "with open('resume_proj_code.txt', 'w', encoding='utf-8') as output_file:\n",
        "        contents = []\n",
        "        for repo in repo_info:\n",
        "            contents.extend(get_repo_contents(repo['owner_login'], repo['repo_name'], headers=headers))\n",
        "\n",
        "            # Write contents to file\n",
        "            for file_info in contents:\n",
        "                output_file.write(f\"\\n--- File: {file_info['path']} ---\\n\")\n",
        "                output_file.write(file_info['content'])\n",
        "                output_file.write(\"\\n\\n\")\n",
        "\n",
        "            output_file.write(f\"\\n{'='*80}\\n\")\n",
        "\n",
        "print(\"Repository contents have been saved to resume_proj_code.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iv1Wr9gcyoCY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Read API key from file\n",
        "with open('openaiAPI.txt', 'r') as api_file:\n",
        "    openai_api_key = api_file.read().strip()\n",
        "\n",
        "# Initialize OpenAI model with lower-cost model\n",
        "llm = ChatOpenAI(\n",
        "    api_key=openai_api_key,\n",
        "    model=\"gpt-3.5-turbo\",  # Lower-cost, token-friendly model\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "def chunk_large_file(input_file, max_chunk_size=10000):\n",
        "    \"\"\"\n",
        "    Split large file into manageable chunks\n",
        "\n",
        "    Args:\n",
        "        input_file (str): Path to large code file\n",
        "        max_chunk_size (int): Maximum characters per chunk\n",
        "\n",
        "    Returns:\n",
        "        list: Chunks of code\n",
        "    \"\"\"\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Split into chunks\n",
        "    chunks = []\n",
        "    for i in range(0, len(content), max_chunk_size):\n",
        "        chunks.append(content[i:i+max_chunk_size])\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def generate_code_descriptions(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Generate descriptions for large code files by processing in chunks\n",
        "    \"\"\"\n",
        "    # Create a prompt template for code description\n",
        "    code_desc_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are an expert code analyst. Provide a 5-sentence description of the code of project. \"\n",
        "                   \"Mention the inferred tech stack.\"),\n",
        "        (\"human\", \"Analyze this code project:\\n\\n{code}\")\n",
        "    ])\n",
        "\n",
        "    # Create output parser\n",
        "    output_parser = StrOutputParser()\n",
        "\n",
        "    # Combine prompt, model, and parser into a chain\n",
        "    code_desc_chain = code_desc_prompt | llm | output_parser\n",
        "\n",
        "    # Chunk the large file\n",
        "    code_chunks = chunk_large_file(input_file)\n",
        "\n",
        "    # Generate descriptions for each chunk\n",
        "    project_descriptions = []\n",
        "    for idx, chunk in enumerate(code_chunks, 1):\n",
        "        try:\n",
        "            description = code_desc_chain.invoke({\"code\": chunk})\n",
        "            project_descriptions.append(f\"### Code Chunk {idx} Description:\\n{description}\\n\")\n",
        "        except Exception as e:\n",
        "            project_descriptions.append(f\"### Code Chunk {idx} Error: {str(e)}\\n\")\n",
        "\n",
        "    # Write descriptions to output file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.writelines(project_descriptions)\n",
        "\n",
        "generate_code_descriptions('resume_proj_code.txt', 'code_desc.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aGm-EJ7yoCZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Read API key from file\n",
        "with open('openaiAPI.txt', 'r') as api_file:\n",
        "    openai_api_key = api_file.read().strip()\n",
        "\n",
        "# Initialize OpenAI model with lower-cost model\n",
        "llm = ChatOpenAI(\n",
        "    api_key=openai_api_key,\n",
        "    model=\"gpt-3.5-turbo\",  # Lower-cost, token-friendly model\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "# Read file content as a list of lines\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.readlines()\n",
        "\n",
        "# Prompt 1: Compare projects with code description\n",
        "def compare_projects_with_code_desc(code_desc_text, projects_file):\n",
        "    \"\"\"\n",
        "    Use LangChain to analyze the code description and compare it with the projects in projects.txt.\n",
        "    LangChain will directly compare each project with the description and output the result.\n",
        "    \"\"\"\n",
        "    # Read content from projects.txt\n",
        "    projects = read_file(projects_file)\n",
        "\n",
        "    # Define the prompt template for comparing with project descriptions\n",
        "    projects_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are an expert code analyst. Compare the content in projects.txt with the content in code_desc.txt. The code_desc.txt was generated by processing large code file in chunks, hence the chunk number is mentioned, so ignore all lines which have chunks mentioned in it.\n",
        "                        Go through the whole file and print everything that is present in the projects.txt file as it is, then verify True/False whether the entire project matches the description of what's given in code_desc.txt. Save the final part in comparison_report.txt.\"\"\"),\n",
        "        (\"human\", \"Analyze this code description and verify the projects:\\n\\n{description}\")\n",
        "    ])\n",
        "\n",
        "    # Create output parser\n",
        "    output_parser = StrOutputParser()\n",
        "\n",
        "    # Combine prompt, model, and parser into a chain\n",
        "    projects_chain = projects_prompt | llm | output_parser\n",
        "\n",
        "    # Execute the LangChain chain for project comparison\n",
        "    try:\n",
        "        result = projects_chain.invoke({\"description\": code_desc_text})\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error during project comparison: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Prompt 2: Compare skills and tech stack with code description\n",
        "def compare_skills_with_code_desc_and_github_lang(code_desc_text, skills_file, github_lang_file):\n",
        "    \"\"\"\n",
        "    Use LangChain to analyze the code description and compare it with the skills in skills.txt and github_lang.txt.\n",
        "    LangChain will directly compare each skill and tech stack with the description and output the result.\n",
        "    \"\"\"\n",
        "    # Read content from skills.txt and github_lang.txt\n",
        "    skills = read_file(skills_file)\n",
        "    github_langs = read_file(github_lang_file)\n",
        "\n",
        "    # Define the prompt template for comparing with skills and tech stack\n",
        "    skills_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are an expert code analyst. Compare the content in skills.txt with the tech stack in code_desc.txt and the languages in github_lang.txt. The code_desc.txt was generated by processing large code file in chunks, hence the chunk number is mentioned, so ignore all lines which have chunks mentioned in it.\n",
        "                        Go through the whole file and print everything that is present in the skills.txt file as it is, then verify True/False whether the entire skills match the description of what's given in tech stack in code_desc.txt and languages in github_lang.txt. Append the final part in comparison_report.txt.\"\"\"),\n",
        "        (\"human\", \"Analyze this code description and verify the skills and tech stack:\\n\\n{description}\")\n",
        "    ])\n",
        "\n",
        "    # Create output parser\n",
        "    output_parser = StrOutputParser()\n",
        "\n",
        "    # Combine prompt, model, and parser into a chain\n",
        "    skills_chain = skills_prompt | llm | output_parser\n",
        "\n",
        "    # Execute the LangChain chain for skills and tech stack comparison\n",
        "    try:\n",
        "        result = skills_chain.invoke({\"description\": code_desc_text})\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error during skills and tech stack comparison: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def generate_report(code_desc_file, projects_file, skills_file, github_lang_file, output_report_file):\n",
        "    \"\"\"\n",
        "    Generate a report by processing the full description and comparing it with projects and skills.\n",
        "    \"\"\"\n",
        "    # Read the full code description from code_desc.txt\n",
        "    code_desc_text = ''.join(read_file(code_desc_file))\n",
        "\n",
        "    # Compare code description with projects\n",
        "    comparison_report_projects = compare_projects_with_code_desc(code_desc_text, projects_file)\n",
        "\n",
        "    # Compare code description with skills and github languages\n",
        "    comparison_report_skills = compare_skills_with_code_desc_and_github_lang(code_desc_text, skills_file, github_lang_file)\n",
        "\n",
        "    # Write the results to the output file\n",
        "    with open(output_report_file, 'w', encoding='utf-8') as file:\n",
        "        file.write(\"Projects Comparison:\\n\")\n",
        "        file.write(comparison_report_projects)\n",
        "        file.write(\"\\n\\nSkills and Tech Stack Comparison:\\n\")\n",
        "        file.write(comparison_report_skills)\n",
        "\n",
        "    # Specify the input and output files directly\n",
        "generate_report('code_desc.txt', 'projects.txt', 'skills.txt', 'github_lang.txt', 'comparison_report.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcW42fPsyoCZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
